\documentclass[11]{article}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage[margin=0.8in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{tikz}
\usetikzlibrary{trees}
\usepackage{verbatim}
\usepackage[hidelinks]{hyperref}
\input{macro}
\usepackage{algorithm,algpseudocode}

\setcounter{MaxMatrixCols}{20}

% Non-trivial normal subgroups / ideals
\newcommand{\trianglerightneq}{\mathrel{\ooalign{\raisebox{-0.5ex}{\reflectbox{\rotatebox{90}{$\nshortmid$}}}\cr$\triangleright$\cr}\mkern-3mu}}
\newcommand{\triangleleftneq}{\mathrel{\reflectbox{$\trianglerightneq$}}}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\newcommand{\z}{\mathbb{Z}}
\newcommand{\n}{\mathbb{N}}
\newcommand{\q}{\mathbb{Q}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\zp}{\mathbb{Z}_p}
\newcommand{\emod}[1]{\underset{#1}{\equiv}}
\newcommand{\per}[1]{\left(#1\right)}
\newcommand{\normal}{\vartriangleleft}
\newcommand{\normalneq}{\triangleleftneq}
\newcommand{\nor}[1]{||#1||}
\newcommand{\intring}{\mathcal{O}}
\newcommand{\partiald}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\bigslant}[2]{{\raisebox{.2em}{$#1$}\left/\raisebox{-.2em}{$#2$}\right.}}
\newcommand{\gen}[1]{\langle #1\rangle}
\newcommand{\sdt}[2]{#1\rtimes_\theta #2}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}
\makeatletter
\providecommand*{\cupdot}{%
  \mathbin{%
    \mathpalette\@cupdot{}%
  }%
}
\newcommand*{\@cupdot}[2]{%
  \ooalign{%
    $\m@th#1\cup$\cr
    \hidewidth$\m@th#1\cdot$\hidewidth
  }%
}
\makeatother

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\allowdisplaybreaks

\pagestyle{fancy}
\rhead{Paper Sketch}
\lhead{DeRandomizing SZK}

\begin{document}
\lecture{}{10/10/20}{Paper Sketch}{Eyal Golombek}

\section{Abstract}

\section{Definitions, Notations and Prelimineris}

\section{Derandomizing $\AM \cap co-\AM$}

\subsection{Random hash function}

\definition{Soundness preserving PRG}

Given some length extending function $G:\{0,1\}^{S(n)} \to \{0,1\}^{R(n)}$ for $S(n) < R(n)$ we say that $G$ is $(\varepsilon, n^d, C(n),k)$-Soundness Preserving $\PRG$ if for every $\IP[k]$ protocol $<P,V>$ such that running time of $V$ is bounded by $n^d$, and the number of random coins $V$ uses is $R(n)$, and the length of the communication $V \to P$ is $C(n)$ the following holds:

Denote with $V^G$ the derandomized verifier, than:

For every $x \notin L$ and for every $P^*$ determenistic \footnote{The reason we care only about deterministic proovers is that the best prover strategy can always be transformed to a determinstic strategy. This will pose a problem when dealing with $\SZK$, however we will be dealing with $\HVSZK$. See \cite{DL20} for details.} prover strategy:
	
$$ |\Pr_{s \in U_s}[<P^*,V^G>(x, r) = 1] - \Pr_{r \in U_r}[<P^*, V>(x, s) = 1]| \leq \varepsilon$$
	

\claim {Collections of PRGs from Hashing} (Paraphrasing of Corollary 5.4 of \cite{AASY16})

For every constant $c > 1$ and every shrinking function $l(r) < r$, the following holds. With probabilty $1-r^{-c}$, a random $t=r^{c+1}$ -wise independent hash function $h: \{0,1\}^s \to \{0,1\}^r$ with input length $s = 2l(r) + (2c + 1)\log(r)$ is a $2^{-l(r)}r^{-c}$ - $\PRG$ against $r^c$-size circuits. We denote the hash family with $\mathcal{H}: \{h_z: \{0,1\}^s \to \{0,1\}^r\}$

\begin{corollary}[Soundness preserving $\PRG$ fron random hash]
\label{PRG from random hash}

Given $\mathcal{H}: \{h_z:\{0,1\}^{2C(n) + (2d+1)\log(R(n))} \to \{0,1\}^{R(n)}\}$ a set of $R(n)^{d+1}$-wise independent hash functions, than a random $h_z \hookleftarrow \mathcal{H}$ is a $(2R(n)^{-d}, R(n)^{d}, C(n))$-Soundness preserving $\PRG$ with error probability of $R(n)^{-d}$.

\end{corollary}

\begin{proof}

We know that a random $h_z \hookleftarrow \mathcal{H}$ as stated above is a $2^{-C(n)}R(n)^{-d}$-$\PRG$ against $R(n)^d$ bounded circuits with probability $1 - R(n)^{-d}$.

Let $<P,V>$ be some interactive proof for some $L \in \IP[k]$, such that $V$ uses $R(n)$ random coins and sends in total $C(n)$ bits of communication to $P$ during the interaction. Assume that the running time of $V$ is bounded by $R(n)^d$. Let $P^*$ be some deterministic prover strategy and let $x \in \{0,1\}^n$ be some shared input.

For any $z$ such that $h_z \hookleftarrow \mathcal{H}$ is indeed a $2^{-C(n)}R(n)^{-d}$ $\PRG$ against $R(n)^d$ circuits, we will show that the following distributions are $(R(n)^d, 2R(n)^{-d})$ computationally indistiguishable:

\begin{enumerate}
 \item $View_V(x,U_r)$ - The view of $V$ when interacting with $P^*$ on a proof over shared input $x$ when the randomness of $V$ is drawn from the uniform distribution $U_r$.
 
 \item $View_V(x, h_z(U_s))$ - The view of $V$ when interacting with $P^*$ on a proof over shared input $x$ when the randomness of $V$ is drawn as the output of $h_z$ over the uniform distribution $U_s$. \footnote{This is \textbf{not} the view of the derandomized verifer $V_{h_z}$ since this verifier would also have the seed as part of it's view}
\end{enumerate}

Assume towards contraditction that there exists some distinguisher $D$ wich is a TM of running time at most $R(n)^d$ that can distinguish theese two distributions with advantage better than $2R(n)^-d$. Thus:

$$ |\Pr_{r \hookleftarrow U_R}[D(View_V(x, r)) = 1] - \Pr_{s \hookleftarrow U_s}[D(View_V(x, h_z(s)) = 1]| \geq 2R(n)^{-d}$$

We denote $\vec{a}$ the messages $V$ sent $P$ during an interaction $\vec{a} = (v_0, v_1, v_2,..., v_{k/2})$ and we denote $\vec{b}$ the messages $P^*$ sent $V$ during the interaction. $\vec{b} = (p_0, p_1, p_2,..., p_{k/2})$ \footnote{W.L.G we can assume the amount of rounds is even. Otherwise we add a null round.}

Since $P^*$ is determinstic, than the message generating function of $P^*$ is dependant solely on the messages sent $V \to P$. Thus the amount of different possible conversations between $V$ to $P^*$ is the amount of different messages that $V$ can send to $P$ during the interaction. In other words $p_i$ is a function of $x$ and $a_0,...,a_i$ only.

Since the communication between $V$ to $P$ is bounded by $C(n)$ we get:

$$ \bigg| \sum_{\hat{a} \hookleftarrow \{0,1\}^{C(n)}}\
\bigg(\
\Pr_{r \hookleftarrow U_R} [D(x, r,\vec{a}, \vec{b}(x,\vec{a})) = 1 | \vec{a}=\hat{a}]\Pr_{r \hookleftarrow U_r}[\vec{a}=\hat{a}]$$
$$- \Pr_{s \hookleftarrow U_S} [D(x, h_z(s),\vec{a}, \vec{b}(\vec{a})) = 1 | \vec{a}=\hat{a}]\Pr_{s \hookleftarrow U_S}[\vec{a}=\hat{a}]\
\bigg) \bigg| \geq 2R(n)^{-d}$$

We have a sum of $C(n)$ elements. Using the triangle inequality we know that there should be at lesat one element $\tilde{a}$ such that:

$$ \bigg| \Pr_{r \hookleftarrow U_R} [D(x, r,\vec{a}, \vec{b}(x,\vec{a})) = 1 | \vec{a}=\tilde{a}]\Pr_{r \hookleftarrow U_r}[\vec{a}=\tilde{a}]$$
$$- \Pr_{s \hookleftarrow U_S} [D(x, h_z(s),\vec{a}, \vec{b}(\vec{a})) = 1 | \vec{a}=\tilde{a}]\Pr_{s \hookleftarrow U_S}[\vec{a}=\tilde{a}]\
\bigg| \geq \frac{2R(n)^{-d}}{2^{C(n)}}$$

We denote $\tilde{b}$ the (unique) vector of responses of the prover $P^*$ when interacting with the verifier on messages $\tilde{a}$.

Now we can define a new turing machine $D'_{\tilde{a}, \tilde{b}}: \{0,1\}^{R(n)} \to \{0,1\}$. On a given input $w \in \{0,1\}^{R(n)}$, the distinguisher $D'_{\tilde{a}, \tilde{b}}$ will run the message generating function of $V$ on $x$ and using $w$ as the random coins, and will emulate $P^*$ responses using $\tilde{b}$. Denote $\vec{e}$ the emulated conversation. if $\vec{e} \neq (\tilde{a}, \tilde{b})$ than $D_{\tilde(a), \tilde(b)}$ rejects. Otherwise $D_{\tilde{a}, \tilde{b}}$ return $D(x, w, \tilde{a}, \tilde{b})$.

The running time of $D_{\tilde{a}, \tilde{b}}$ is comprised of emulating $V$ which runs in $R(n)^d$ time, and running $D$ which is also bounded by $R(n)^d$, thus $D_{\tilde{a}, \tilde{b}}$ is time bounded by $R(n)^d$.

Clearly $D_{\tilde{a}, \tilde{b}}$ can distinguish $U_{R(n)}$ from $h_z(U_{S(n)})$ with advantage $2R(n)^{-d} 2^{-C(n)}$ which is a contradiction to the fact that $h_z$ is a $2^{-C(n)} R(n)^{-d}$ - $\PRG$ against $R(n)^d$ bounded circuits.

In conclusion we have shown that $View_V(x,U_r)$ and $View_V(x, h_z(U_s))$ are $(R(n)^d, 2R(n)^{-d})$ computationally close. We denote with $V_{acc}$ the predicate that given an interaction between $P$ and $V$ will decide wether $V$ should accept or reject. Clearly $V_acc$ is time bound by $R(n)^d$ and so is foolded by the two indistinguishable distributions.

To sum it all up:

$$ \bigg| \Pr_{s \in U_{S(n)}} [<P^*, V^{h_z}>(x, r) = 1] - \Pr_{r \in U_{R(n)}}[<P^*, V>(x,r) = 1] \bigg| = $$
$$ \bigg| \Pr_{s \in U_{S(n)}} [V_{acc}(View_V(x, h_z(s))) = 1] - \Pr_{r \in U_{R(n)}} [V_{acc}(View_V(x, h_z(r))) = 1] \bigg| \leq 2R(n)^{-d}$$

This is true for any strategy $P^*$ and any shared input $x$, for any $z$ such that $h_z$ is indeed a strong $\PRG$. Such $z$ are drawen with probability $1 - R(n)^{-d}$ and for such those $z$ we get that $h_z$ is a $(R(n)^{-d}, R(n)^d, C(n))$ Soundness preserving $\PRG$ as required.

\end{proof}

\definition {Computational $\varepsilon$-$\PRG$}

Given a class of functions $\mathcal{F} \subset \{f | f: \{0,1\}^n \to \{0,1\}\}$ we say that $G: \{0,1\}^r \to \{0,1\}^n$ is an $\varepsilon$ computational $\PRG$ against $\mathcal{F}$ if for every $f \in \mathcal{F}$:
$$ \bigg| \Pr_{r \in U_r}[C(G(r)) = 1] - \Pr_{n \in U_n}[C(n) = 1] \bigg| \leq \varepsilon$$

\begin{claim} {Collection of computational $\PRG$s from hash functions}

Let $\mathcal{F} \subset P(\{0,1\}^n)$ be some set of sets of binary strings of length $n$ \footnote{One can think of a set of binary functions as simply a set of the strings that are mapped to 1}. Denote $F = |\mathcal{F}|$. Let $\varepsilon, \delta \in [0,1]$ be two constants. Denote with $\mathcal{H} = \{ h_z : \{0,1\}^r \to \{0,1 \}^n \}$ a family of hash functions from $\{0,1\}^r$ to $\{0,1\}^n$. If $\mathcal{H}$ is t-wise independent hash functions family , than it is also a collection of $\varepsilon$ computational $\PRG s$ against $\mathcal{F}$ with failure probability $\delta$. Where $r = 2\log(1 / \varepsilon) + 3\log(t)$ and $t = 2\log(1 / \delta) + 2\log (F)$.
\end{claim}

\begin{proof}

Let $\sigma \in \mathcal{F}$ be some set in the family $\mathcal{F}$. For every $s \in \{0,1\}$ we define a random variable $v_s$ such that upon selecting $h \hookleftarrow \mathcal{H}$ uniformly at random, $v_s$ is the indicator for the event $h(s) \in \sigma$. Since $\mathcal{H}$ is a family of t-wise hash functions we know that $h(s)$ is distributed uniformly in $\{0,1\}^n$ (Where the randomness is taken from the selction of $h$). Thus we get that:
$$ E[v_s] = \Pr_{h \hookleftarrow \mathcal{H}}[h(s) \in \sigma] = \Pr_{r \in U_n}[r \in \sigma] = \mu_{\sigma}$$

Where $\mu_{\sigma}$ is the density of $\sigma$ in $\{0,1\}^n.$

Our goal is to show that a random function is indeed a $\varepsilon$ Computationsl $\PRG$ so we will examine the value:

$$ \Delta := \Pr_{h \hookleftarrow \mathcal{H}}[ | \Pr_{s \in U_s}[G(s) \in \sigma] - \Pr_{r \in U_n}[r \in \sigma] | \leq \varepsilon ] = \Pr_{h \hookleftarrow \mathcal{H}}[ | \sum_{s \in \{0,1\}^r} v_s 2^{-r}- \mu_{\sigma} | \leq \varepsilon] $$

Using the boundry on sums of $t$ wise independent variables we get:

$$ \Delta  \leq \left( \frac{t^2}{2^{r} \varepsilon^2} \right) ^{t/2} = \left(\frac{1}{t} \right)^{t/2} \leq \frac{\delta}{|F|}$$

Thus if we take a union bound over all $\sigma \in \mathcal{F}$ we get that for all $\sigma \in \mathcal{F}$, a random hash function from the family $\mathcal{H}$ is an $\varepsilon$ Computational $\PRG$ for $\sigma$ with probability $1-\delta$

\end{proof}

\subsection{Sampling an hash function at random - Efficient?}

\textbf{TODO:}
Explain why we can uniformly select hash functions, and there is one algorithm to generate the hash function for any $z$.

Explain the low-degree polynomials, and how random complex is choosing one at random.

\subsection{De-Randomizing the hash function selection}

\begin{definition}{Protocol Specific - Soundness preserving PRG}

Let $<P,V>$  be some $\IP[k]$ protocol for some constant $k$ such that $V$ uses $R(n)$ random coins and has running time bounded by $R(n)^d$ and sends $C(n)$ bits of random to $P$ during the interaction. We say that a length extending function $G:\{0,1\}^{S(n)} \to \{0,1\}^{R(n)}$ for $S(n) < R(n)$ is a $\varepsilon$-Soundness Preserving $\PRG$ for $<P,V>$ if:

$$ \forall{x \notin \{0,1\}^n}. \forall{P^*}. \bigg|\Pr_{s \in U_s}[<P^*,V^G>(x, r) = 1] - \Pr_{r \in U_r}[<P^*, V>(x, s) = 1]\bigg| \leq \varepsilon$$
	
\end{definition}

Given some specific $\IP[k]$ protocol $<P,V>$ for language $L$ such that $L \in \AM \cap co\AM$, with parmaters $R(n), C(n), R(n)^d$,  let $\mathcal{H}_{C,d}$ be a family of hash functions that satisfies \ref{PRG from random hash} and given the evaluation algorithm $H: \{0,1\}^{Z(n)} \times \{0,1\}^{S(n)} \to \{0,1\}^{R(n)}$ which evaluates to $H(z, \cdot) = h_z(\cdot)$ and runs in polynomial time:
We define a promise problem $\Pi_{C,d,R}^{P,V}$ over strings $z$ such that:
\begin{itemize}
	\item $z \in \Pi_{YES}$ if $H(z, \cdot)$ is not an $4R(n)^{-d}$ soundness preserving $\PRG$ for $<P,V>$.
	\item $z \in \Pi_{NO}$ if $H(z, \cdot)$ is a $2R(n)^{-d}$ soundness preserving $\PRG$ for $<P,V>$
\end{itemize}

\begin{claim} {$\Pi_{C,d,R}$ is in $\AM$}

For any constant $d > 1$ and length extending funcions $R(n), C(n)$, and for any $\IP[k]$ protocol $<P,V>$ for some constant $k$ with negligable soundness $\mu(n)$ \footnote{We will tackle the case of original non-negligable soundness in the future}, such that $V$ runs in time $R(n)^d$, uses $R(n)$ random coins and sends $C(n)$ bits to $P$ during the intearction, the gap problem $\Pi_{C,d,R}^{P,V}$ is in $\AM$.

\end{claim}

\begin{proof}

For any such $d$ and $<P,V>$ we will define an $\AM$ protcol that accepts $\Pi_{C,d,R}^{P,V}$. On any shared input $z \in \{0,1\}^*$ Merlin will try to prove that $h_z$ is not a good soundness preserving $\PRG$. Merlin will do that by presenting the one $x \notin L$ that $V$ can be cheated on, and will emulate the cheating strategy.


\begin{algorithm}
\caption{$\AM$ protocol for $\Pi_{C,d,R}$ on shared input $(1^n, z)$}
\begin{algorithmic}[1]

\State $M$ sends $A$ some $x \in \{0,1\}^n$
\State $M$ and $A$ perform an interactive proof for the fact that $x \notin L$. $M$ tries to convince $A$ that $x \notin L$, and if $A$ is not convinces, $A$ will reject.
\State $A$ uses $h_z$ and random seed $s$ to generate $r = h_z(s)$.
\State $A$ and $M$ play the interactive proof $<V, P^*>$ on $x$ while $A$ runs $V$ using the random coins $r$, and $M$ plays the strategy $P^*$ of his likings.
\State $A$ accepts if $V$ accepts the interaction with $P^*$ on $x$ and $r$.

\end{algorithmic}
\end{algorithm}

\underline{Completness:}
Given some $z$ such that $h_z$ is not a $4R(n)^d$ soundness preserving $\PRG$ for $<P,V>$, than clearly there exists some $x \notin \{0,1\}^n$ and some strategy $P^*$ such that  

$$\bigg|\Pr_{s \in U_s}[<P^*,V^G>(x, r) = 1] - \Pr_{r \in U_r}[<P^*, V>(x, s) = 1]\bigg| \geq 4R(n)^d$$

Since the original soundness of the protocol $<P,V>$ is $\mu(n)$ which is negligable in $n$, we get that the probablity that $V$ will accept the interaction with $P^*$ if given the psuedo-coins is large enough:$$\Pr_{s \in U_s}[<P^*,V^G>(x, r) = 1] > 4R(n)^d$$.

Morover we assumed that $L \in \AM \cap co\AM$, so $M$ can prove to $A$ that $x \notin L$ in a constant amount of rounds, and do so with negligable completness error $\mu(n)$.

In conclusion, the probabilty that $A$ will accept the interaction is at least $(1-\mu(n))4R(n)^{-d}$.

\underline{Soundness:}
Given some $z$ such that $h_z$ is a $R(n)^d$ soundness preserving $\PRG$ for $<P,V>$ we examine the acceptance probabilty of $A$. 
\begin{itemize}
	\item A cheating merlin $M^*$ can either send some $x$ such that $x \in L$ and try to convince $A$ that $x \notin L$. Since $L \in co\AM$ the probability that $M^*$ succeedes is no more than $\mu(n)$.
	
	\item A cheating merlin $M^*$ can send some $x \notin L$, and than play some strategy $P^*$ against $V^G$. Since $x \notin L$ we know that probability that $V$ will accept an interaction with $P^*$ on real coins is at most $\mu(n)$. Since $h_z$ is a soundness preserving $\PRG$ we get that $V^G$ will accept interaction with $P^*$ with probabilty at most $\mu(n) + 2R(n)^{-d}$
\end{itemize}

In conclusion, for any $z \in \Pi_{NO}$ and any cheating merlin $M^*$ the probabilty that $A$ will accept is at most $2\mu(n) + 2R(n)^{-d}$.

\begin{theorem}


TODO: Simultanously derandomize both $\NP / poly$ circuits to get a $\PRG$ both for soundness and completness, thus derandomizing the $\AM \cap co\AM$ class.

\end{theorem}

\end{proof}

\section{Why tackle $\HVSZK$ and not $\SZK$?}
\textbf{TODO:}
\begin{enumerate}
	\item Show that if the original language is in $\SZK$ than clearly the de-randomized one is also simulatable. (Naive)
	\item Explain why it's unlikely to have a gap between randomness and comunication in a $\SZK$ proof system, since all known transformation first kill that gap. (Add analysis of known transformations)

	
\end{enumerate}


\section{Inverting the $\PRG$}
\textbf{TODO:}
Explain why it's OK for the $\PRG$ to also output part of the seed (The part the selects the hash function)

Explain how to invert low-level polynomials in polynomial time and generate an inverting algorithm


\section{Knowledge complexity Vs Randomness tradeoff}
\textbf{TODO:}
Talk about knowledge complexity in the fraction sense. Show how to create a simulator to a fraction of the conversations.

Show that the simulation deviation is not a problem due to the previous section about $\varepsilon$ - $\PRG$.

\subsection{Borrowing randomness from the proover}
\textbf{TODO:}
Show that we can play with the tradeoff acheived above, by requesting the prover to add ranomness to the interaction, and thus increase communication, but loose less "knowledge"

\subsection{Knowledge complexity and sequential repetitions}
\textbf{TODO:}
A-priori, knowledge complexity isn't preserved over sequential repetitions of the protocol. Perhaps we can show that in our case it is? Or is it?

\section{Why can't we get Zero-Knowoledge?}
	\item Hopefully be able to show that it will be imposible to simulate the new verifier using the old simulator as black-box.	

\section{Super-Constant number of rounds}

\section{Computational Settings}

\subsection{Trusted Hardware}

\begin{thebibliography}{}
\bibitem{AASY16} B. Applebaum, S. Aremenko, R. Shaltiel, G. Yang: Incompressible Functions, Relative-Error Extractors, and the Power
of Nondeterministic Reductions

\bibitem{DL20} H. Dahari, Y. Lindel: Deterministic-Prover Zero-Knowledge Proofs

\end{thebibliography}



\end{document}



